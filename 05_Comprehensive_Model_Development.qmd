---
title: "Home Credit — EDA & Modeling"
subtitle: "Phase 2: Comprehensive Model Development & Selection"
author: "Whitney Bullock · Thomas Beck · Caleb Call"
date: "November 19, 2025"
format:
  html:
    embed-resources: true
    theme: flatly
    highlight-style: tango
    toc: true
    toc-depth: 3
    toc-location: left
    number-sections: false
    df-print: paged
    code-fold: true
    code-tools: true
  pdf:
    documentclass: article
    geometry:
      - margin=1in
    toc: true
    toc-depth: 3
    number-sections: false
    colorlinks: true
    mainfont: "Arial"
    sansfont: "Arial"
    monofont: "Courier New"
editor_options:
  chunk_output_type: inline
execute:
  warning: false
  message: false
  echo: true
---

```{r setup, include=FALSE}
# -----------------------------------------------------------------------------
# Setup and set global chunk options
# -----------------------------------------------------------------------------
knitr::opts_chunk$set(
  echo = TRUE,           # Must be TRUE for code folding to work in HTML
  warning = FALSE,       # Suppress warnings for professional output
  message = FALSE,       # Suppress package loading messages
  error = FALSE,         # Suppress errors
  fig.align = "center",
  fig.width = 10,        
  fig.height = 6,        
  dpi = 300,
  out.width = '100%'    
)

# Global Reproducibility Options
set.seed(1234)
options(dplyr.summarise.inform = FALSE, pillar.sigfig = 3, scipen = 999)

# Prevent "Hit <Return> to see next plot"
options(device.ask.default = FALSE)
```

```{r libraries, include=TRUE, results='asis'}
# -----------------------------------------------------------------------------
# Library Management
# -----------------------------------------------------------------------------

# Required packages
required_libraries <- c(
  "tidyverse", "skimr", "DataExplorer", "corrplot", "naniar",
  "caret", "tidymodels", "xgboost", "kableExtra", "Matrix",
  "VennDiagram", "grid", "rpart", "rpart.plot", "pROC",
  "randomForest", "ranger", "knitr", "broom", "foreach", 
  "parallel", "scales", "Ckmeans.1d.dp"
)

# Install missing packages automatically
missing_pkgs <- setdiff(required_libraries, rownames(installed.packages()))
if (length(missing_pkgs) > 0) {
  install.packages(missing_pkgs, quiet = TRUE)
}

# Load all libraries
invisible(lapply(required_libraries, library, character.only = TRUE))

# Print loaded libraries for reproducibility record
cat("**Loaded Libraries:**\n\n")
cat(paste0("[", paste(required_libraries, collapse = ", "), "]"))
```

```{r configuration, include=FALSE}
# -----------------------------------------------------------------------------
# Configuration & Helpers
# -----------------------------------------------------------------------------

# 1. Hardware Optimization Toggle
# TRUE = Fast (Parallel) - Use for checking for errors
# FALSE = Reproducible (Single Thread) - Use for final knitting/submission
ENABLE_PARALLEL <- FALSE 

if (ENABLE_PARALLEL) {
  if (!require("doParallel")) install.packages("doParallel")
  library(doParallel)
  
  # Detect available cores
  detected_cores <- parallel::detectCores(logical = FALSE)
  # Use all physical cores but keep one free for the OS
  # We assign this to 'all_cores' so downstream models use it automatically
  all_cores <- ifelse(detected_cores > 1, detected_cores - 1, 1)
  
  registerDoParallel(cores = all_cores)
  message(paste0("Parallel processing enabled with ", all_cores, " cores."))
  
} else {
  # Force sequential processing for absolute reproducibility
  if (!require("foreach")) install.packages("foreach")
  foreach::registerDoSEQ()
  
  # CRITICAL: Set 'all_cores' to 1. 
  # This forces ranger and xgboost to run single-threaded, ensuring 
  # the random seed (1234) produces identical numbers every time.
  all_cores <- 1
  
  message("Sequential processing enabled for reproducibility.")
}

# 2. Visualization Settings
par(ask = FALSE)
theme_set(theme_minimal(base_family = "sans"))

# 3. Outputs & Figures Directory
if (!dir.exists("outputs")) dir.create("outputs")
if (!dir.exists("fig")) dir.create("fig")

# 4. Color Palette 
col_good    <- "steelblue"    # Low Risk / Repaid
col_bad     <- "firebrick"    # High Risk / Default
col_neutral <- "lightgrey"    # Neutral
col_line1   <- "darkorange"   # Model 1 Line
col_line2   <- "forestgreen"  # Model 2 Line
col_line3   <- "purple"       # Model 3 Line

# 5. Table Formatting
print_kbl <- function(data, caption = "Table Output") {
  data %>%
    head(10) %>%
    knitr::kable(digits = 3, caption = caption, booktabs = TRUE, 
                 format.args = list(big.mark = ",")) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
    row_spec(0, bold = TRUE, color = "white", background = "#2c3e50")
}

# 6. Tree Plot Formatting
plot_professional_tree <- function(model) {
  rpart.plot::rpart.plot(
    model,
    type = 0,             
    extra = 104,              
    under = TRUE,             
    fallen.leaves = TRUE,     
    box.palette = "BuOr",   
    varlen = 0,               
    faclen = 0,          
    roundint = FALSE,         
    digits = 2,               
    tweak = 1.1,              
    clip.right.labs = FALSE,  
    branch.lty = 1,           
    shadow.col = 0,        
    main = "Decision Rules for Default Risk Prediction"
  )
}
```

# Business Problem

Home Credit’s operational model is fundamentally about financial inclusion. Loans are provided to populations with little or no credit history—groups often rejected by traditional banks. This creates a risk assessment challenge: evaluating repayment likelihood without standard credit scores.

The core issue is information asymmetry. Strict approval criteria exclude creditworthy applicants, failing the corporate mission. Lenient criteria result in financial losses from defaults. The goal is not merely prediction accuracy, as only 8% of the population defaults. A basic model could achieve 92% accuracy simply by approving every applicant, but this fails to manage risk. Instead, applicants must be ranked by default likelihood using AUC-ROC. This approach identifies specific "invisible risk" cases for further review.

# Methodology

The standard CRISP-DM framework structures the analysis:

-   **Data Preparation:** Missing data is handled (imputing `EXT_SOURCE_1` with the median), empty columns are removed, and features are engineered to capture age and employment length in years.
-   **Exploratory Data Analysis (EDA):** The 92/8 class imbalance is verified, and external credit scores are identified as the strongest predictors.
-   **Modeling & Evaluation:** We perform a comparative analysis of four distinct algorithms:
    1.  **Logistic Regression:** Establishing a linear baseline.
    2.  **Decision Trees:** Investigating non-linear rule sets.
    3.  **Random Forest:** Reducing variance through bagging.
    4.  **Gradient Boosting (XGBoost):** Optimizing bias through boosting.
-   **Selection:** The model with the highest cross-validated AUC will be selected for the final production pipeline.

# Data Preparation & Feature Engineering

The raw data is imported and a summary of dataset dimensions is generated. We apply the cleaning pipeline established during the EDA phase.

```{r data_import}
# Load Datasets
train <- read.csv("application_train.csv", check.names = FALSE)
test  <- read.csv("application_test.csv",  check.names = FALSE)

# Output Dimensions
tibble(
  dataset = c("train", "test"),
  n_rows  = c(nrow(train), nrow(test)),
  n_cols  = c(ncol(train), ncol(test))
) %>% print_kbl(caption = "Dataset Dimensions")
```

Missing values for `EXT_SOURCE_1` are filled using the median to preserve signal, and columns with greater than 50% missingness are dropped to reduce noise.

```{r missing_imputation}
# Impute missing EXT_SOURCE_1
med <- median(train$EXT_SOURCE_1, na.rm = TRUE)
train$export_source_1_imputed <- ifelse(is.na(train$EXT_SOURCE_1), med, train$EXT_SOURCE_1)

# Drop high-missingness cols
missing_rate <- train |> 
  summarise(across(everything(), ~ mean(is.na(.)))) |> 
  pivot_longer(everything(), names_to = "var", values_to = "rate")
vars_to_drop <- missing_rate |> filter(rate > 0.5) |> pull(var)
train_cleaned <- train |> select(-all_of(vars_to_drop))
```

Data inconsistencies are corrected, such as converting negative days to positive years and fixing a sentinel value in the employment data.

```{r feature_engineering}
# feature engineering
train_cleaned <- train_cleaned |>
  mutate(
    TARGET = as.factor(TARGET), 
    AGE_YEARS = abs(DAYS_BIRTH) / 365,
    DAYS_EMPLOYED_FIXED = ifelse(DAYS_EMPLOYED == 365243, NA, DAYS_EMPLOYED),
    YEARS_EMPLOYED = abs(DAYS_EMPLOYED_FIXED) / 365,
    YEARS_REGISTERED = abs(DAYS_REGISTRATION) / 365,
    YEARS_PUBLISH = abs (DAYS_ID_PUBLISH) / 365
  )
```

# Exploratory Data Analysis

## Target Distribution

```{r target_distribution}
# Calc target dist
tb_target <- train_cleaned |> 
  count(TARGET, name = "count") |>
  mutate(
    class = recode(TARGET, `0` = "Repaid (0)", `1` = "Default (1)"),
    pct = count / sum(count)
  ) |>
  select(class, count, pct)

tb_target %>% print_kbl(caption = "Target Distribution")

# Plot Target Distribution
ggplot(tb_target, aes(x = class, y = pct, fill = class)) +
  geom_col(width = 0.6) +
  geom_text(aes(label = sprintf("%.1f%%", 100 * pct)), vjust = -0.6, size = 4, fontface = "bold", color = "white") +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1)), labels = scales::percent) +
  scale_fill_manual(values = c("Default (1)" = col_bad, "Repaid (0)" = col_good), guide = "none") + 
  labs(title = "Distribution of Target Variable", 
       subtitle = "Significant imbalance: Majority (Blue) Repaid vs Minority (Red) Default",
       x = NULL, y = "Proportion") +
  theme(plot.title.position = "plot",
        panel.grid.major.x = element_blank())
```

As expected, the data is heavily skewed. 91.93% of applicants repay their loans, while only 8.07% default. This reinforces why "accuracy" is an insufficient metric.

## Key Categorical Relationships

Categories with the highest default rates are analyzed to identify areas of risk.

```{r categorical_risk_plot, fig.height=10}
# select categorical features for dashboard
cat_features <- c("NAME_CONTRACT_TYPE", "CODE_GENDER", "FLAG_OWN_REALTY",
                  "NAME_INCOME_TYPE", "NAME_EDUCATION_TYPE", "NAME_HOUSING_TYPE")

# prep data for faceted plot
risk_data <- train_cleaned %>%
  select(all_of(cat_features), TARGET) %>%
  pivot_longer(cols = -TARGET, names_to = "Feature", values_to = "Value") %>%
  group_by(Feature, Value) %>%
  summarise(
    Count = n(),
    Default_Rate = mean(as.numeric(as.character(TARGET))),
    .groups = "drop"
  ) %>%
  filter(Count > 0.01 * nrow(train_cleaned)) # remove rare categories

# plot risk dashboard
ggplot(risk_data, aes(x = reorder(Value, Default_Rate), y = Default_Rate, fill = Default_Rate)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~Feature, scales = "free_y", ncol = 2) +
  scale_fill_gradient(low = col_good, high = col_bad) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Default Risk by Applicant Attributes",
       subtitle = "Risk intensity (Orange) vs. Safety (Teal) across key categories",
       x = NULL, y = "Default Rate") +
  theme(legend.position = "none",
        axis.text.y = element_text(size = 8))
```

## Numeric Correlations

```{r numeric_correlations}
# Calculate numeric correlations
numeric_cols <- train_cleaned |> select(where(is.numeric))
cors <- vapply(numeric_cols, function(x) suppressWarnings(cor(x, as.integer(as.character(train_cleaned$TARGET)), use = "complete.obs")), numeric(1))

corr_tbl <- tibble(Variable = names(cors), Correlation = round(unname(cors), 3)) |>
  arrange(desc(abs(Correlation))) |> slice_head(n = 12)

# Visualize correlations with plot
ggplot(corr_tbl, aes(x = reorder(Variable, abs(Correlation)), y = Correlation, fill = Correlation)) +
  geom_col(width = .6) +
  coord_flip() +
  scale_fill_gradient2(low = col_good, high = col_bad, mid = col_neutral, midpoint = 0) +
  labs(title = "Top Numeric Correlations with Target", 
       subtitle = "Blue = Lower Risk (Negative Correlation) | Red = Higher Risk (Positive Correlation)",
       x = NULL, y = "Pearson r") +
  theme(plot.title.position = "plot", legend.position = "bottom")
```

External source scores are the strongest predictors (negative correlation indicates higher scores correspond to lower risk). Age and employment history also provide significant predictive power.

# Comparative Model Analysis

To determine the best approach for the final production model, we test four algorithms ranging from simple linear models to complex ensemble methods. We use a 70/30 train/test split for validation.

```{r local_split}
# train/test split
set.seed(1234)
train_index <- sample(1:nrow(train_cleaned), 0.7 * nrow(train_cleaned))
train_data  <- train_cleaned[train_index, ]
test_data   <- train_cleaned[-train_index, ]
```

## 1. Logistic Regression (Linear Benchmark)

Logistic Regression serves as our linear baseline. We test three variations to isolate the impact of different feature sets.

```{r logistic_models}
# Helper function for ROC calculation
validate_model <- function(model, test_df, predictors) {
  clean_test <- test_df[complete.cases(test_df[, predictors]), ]
  preds <- predict(model, newdata = clean_test, type = "response")
  actuals <- as.numeric(as.character(clean_test$TARGET))
  roc_obj <- roc(actuals, preds, quiet = TRUE)
  return(list(roc = roc_obj, auc = auc(roc_obj)))
}

# Model 1: Key Predictors
predictors_1 <- c("EXT_SOURCE_3", "EXT_SOURCE_2", "export_source_1_imputed", 
                  "AGE_YEARS", "YEARS_EMPLOYED")
log_model1 <- glm(TARGET ~ EXT_SOURCE_3 + EXT_SOURCE_2 + export_source_1_imputed + 
                    AGE_YEARS + YEARS_EMPLOYED, 
                  data = train_data, family = "binomial")

# Model 2: Gender Only
predictors_2 <- "CODE_GENDER"
log_model2 <- glm(TARGET ~ CODE_GENDER, data = train_data, family = "binomial")

# Model 3: Income Only
predictors_3 <- "AMT_INCOME_TOTAL"
log_model3 <- glm(TARGET ~ AMT_INCOME_TOTAL, data = train_data, family = "binomial")

# evaluate
res1 <- validate_model(log_model1, test_data, predictors_1)
res2 <- validate_model(log_model2, test_data, predictors_2)
res3 <- validate_model(log_model3, test_data, predictors_3)

# Print Summary
cat("Model 1 (Key Predictors) AUC:", round(res1$auc, 4), "\n")
cat("Model 2 (Gender Only)    AUC:", round(res2$auc, 4), "\n")
cat("Model 3 (Income Only)    AUC:", round(res3$auc, 4), "\n")

# roc comparison plot
plot(res1$roc, col = col_line1, lwd = 3, main = "ROC Curves for Logistic Models", grid = TRUE)
plot(res2$roc, col = col_line2, add = TRUE, lwd = 2, lty = 2)
plot(res3$roc, col = col_line3, add = TRUE, lwd = 2, lty = 3)
legend("bottomright", 
       legend = c(paste0("Key Predictors (AUC: ", round(res1$auc, 3), ")"),
                  paste0("Gender Only (AUC: ", round(res2$auc, 3), ")"),
                  paste0("Income Only (AUC: ", round(res3$auc, 3), ")")),
       col = c(col_line1, col_line2, col_line3), 
       lwd = c(3, 2, 2), lty = c(1, 2, 3))
```

**Result:** The best logistic regression model achieved an **AUC of 0.73**. While strong for a linear model, the single-feature models (Gender, Income) performed near-chance (~0.50), confirming that demographics alone are insufficient.

## 2. Decision Tree (Non-Linear Rules)

We use a Decision Tree to capture non-linear relationships. We apply a 5:1 class weight to the minority class to penalize missed defaults.

```{r decision_tree, fig.width=14, fig.height=9}
# data prep for tree
train_tree <- train_data |> 
  rename(EXT_SRC_1 = export_source_1_imputed) |> 
  mutate(TARGET = factor(TARGET, levels = c(0, 1), labels = c("Repay", "Default")))

# apply weights (5:1 for class imbalance)
weights <- ifelse(train_tree$TARGET == "Default", 5, 1)

# train tree
tree_model <- rpart(
  TARGET ~ EXT_SOURCE_3 + EXT_SOURCE_2 + EXT_SRC_1 +
           AGE_YEARS + YEARS_EMPLOYED,
  data = train_tree,
  method = "class",
  weights = weights,
  control = rpart.control(minsplit = 20, cp = 0.001)
)

# plot tree
plot_professional_tree(tree_model)

# validation
predictors <- c("EXT_SOURCE_3", "EXT_SOURCE_2", "export_source_1_imputed",
                "AGE_YEARS", "YEARS_EMPLOYED")
test_clean <- test_data[complete.cases(test_data[, predictors]), ]

test_tree_clean <- test_clean |> 
  rename(EXT_SRC_1 = export_source_1_imputed) |> 
  mutate(TARGET = factor(TARGET, levels = c(0, 1), labels = c("Repay", "Default")))

tree_probs <- predict(tree_model, newdata = test_tree_clean, type = "prob")[, "Default"]

# roc evaluation
actuals <- ifelse(test_tree_clean$TARGET == "Default", 1, 0)
roc_tree <- roc(actuals, tree_probs, quiet = TRUE)
cat("Decision Tree (Weighted) AUC:", round(auc(roc_tree), 4), "\n")

plot(roc_tree, col = col_good, lwd = 2, 
     print.auc = FALSE,
     main = "Decision Tree Performance (AUC: 0.685)", grid = TRUE)
```

**Result:** The weighted Decision Tree achieved an **AUC of 0.685**. It provides interpretability but lacks the predictive power of ensemble methods or the stability of the logistic regression.

## 3. Random Forest (Bagging Ensemble)

We scale up to a Random Forest (500 trees) to reduce variance and improve stability.

```{r random_forest}
set.seed(1234)

# define predictors
predictors_rf <- c("EXT_SOURCE_3", "EXT_SOURCE_2", "export_source_1_imputed",
                   "AGE_YEARS", "YEARS_EMPLOYED")

# train random forest (parallel)
rf_model <- ranger(
  formula = as.formula(paste("TARGET ~", paste(predictors_rf, collapse = "+"))),
  data = train_data,
  num.trees = 500,
  mtry = 3,
  importance = "impurity",
  probability = TRUE,
  num.threads = all_cores
)

# validate
test_rf_clean <- test_data[complete.cases(test_data[, predictors_rf]), ]
rf_pred <- predict(rf_model, data = test_rf_clean)
rf_probs <- rf_pred$predictions[, 2] 

# roc evaluation
actuals_rf <- as.numeric(as.character(test_rf_clean$TARGET))
roc_rf <- roc(actuals_rf, rf_probs, quiet = TRUE)
cat("Random Forest AUC:", round(auc(roc_rf), 4), "\n")

plot(roc_rf, col = col_good, lwd = 2, 
     print.auc = FALSE,
     main = "Random Forest Performance (AUC: 0.651)", grid = TRUE)
```

**Result:** The Random Forest achieved an **AUC of 0.651**. This underperformance (compared to the single tree) is likely due to the lack of explicit class weighting in this specific run, highlighting the sensitivity of bagging algorithms to extreme class imbalance.

## 4. Gradient Boosting (The Champion Model)

Finally, we test Gradient Boosting (XGBoost), which builds trees sequentially to correct errors. This addresses the bias often found in imbalanced datasets.

```{r xgboost_model}
set.seed(1234)

# handle missing values
prev_na_action <- options("na.action")
options(na.action = "na.pass")

# create sparse matrices
features <- setdiff(names(train_data), "TARGET")
train_matrix <- sparse.model.matrix(TARGET ~ . - 1, data = train_data[, c("TARGET", features)])
test_matrix  <- sparse.model.matrix(TARGET ~ . - 1, data = test_data[, c("TARGET", features)])

options(na.action = prev_na_action$na.action)

# label preparation
train_label <- as.numeric(as.character(train_data$TARGET))
test_label  <- as.numeric(as.character(test_data$TARGET))

# weighting for imbalance
neg_count <- sum(train_label == 0)
pos_count <- sum(train_label == 1)
scale_weight <- neg_count / pos_count

# hyperparameters
xgb_params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  eta = 0.1,                
  max_depth = 6,            
  subsample = 0.8, 
  colsample_bytree = 0.8,
  scale_pos_weight = scale_weight, 
  nthread = all_cores       
)

# train xgboost
xgb_model <- xgb.train(
  params = xgb_params,
  data = xgb.DMatrix(data = train_matrix, label = train_label),
  nrounds = 500,
  watchlist = list(train = xgb.DMatrix(data = train_matrix, label = train_label),
                   test  = xgb.DMatrix(data = test_matrix, label = test_label)),
  print_every_n = 50,
  early_stopping_rounds = 20,
  verbose = 0
)

# generate predictions
xgb_probs <- predict(xgb_model, test_matrix)

# roc evaluation
roc_xgb <- roc(test_label, xgb_probs, quiet = TRUE)
cat("Gradient Boosting (XGBoost) AUC:", round(auc(roc_xgb), 4), "\n")

plot(roc_xgb, col = col_good, lwd = 2, 
     print.auc = FALSE,
     main = "XGBoost Performance (AUC: 0.742)", grid = TRUE)
```

**Result:** XGBoost achieves the highest performance with an **AUC of 0.742**. The sequential learning process effectively targets the difficult "hidden risk" cases that other models miss.

# Kaggle Submission

The final XGBoost model is applied to the official test data to generate the submission file.

```{r kaggle_submission}
# load test data
kaggle_test <- read.csv("application_test.csv", check.names = FALSE)

# feature engineering
kaggle_test <- kaggle_test |>
  mutate(
    AGE_YEARS = abs(DAYS_BIRTH) / 365,
    DAYS_EMPLOYED_FIXED = ifelse(DAYS_EMPLOYED == 365243, NA, DAYS_EMPLOYED),
    YEARS_EMPLOYED = abs(DAYS_EMPLOYED_FIXED) / 365,
    YEARS_REGISTERED = abs(DAYS_REGISTRATION) / 365,
    YEARS_PUBLISH = abs(DAYS_ID_PUBLISH) / 365,
    export_source_1_imputed = EXT_SOURCE_1
  )

# matrix creation
if (!exists("features")) features <- setdiff(names(train_data), "TARGET")
kaggle_subset <- kaggle_test[, features]

prev_na <- options("na.action")
options(na.action = "na.pass")
kaggle_matrix_raw <- sparse.model.matrix(~ . - 1, data = kaggle_subset)
options(na.action = prev_na$na.action)

# column alignment
model_cols <- xgb_model$feature_names
missing_cols <- setdiff(model_cols, colnames(kaggle_matrix_raw))

if (length(missing_cols) > 0) {
  missing_mat <- Matrix(0, nrow = nrow(kaggle_matrix_raw), ncol = length(missing_cols), sparse = TRUE)
  colnames(missing_mat) <- missing_cols
  kaggle_matrix_raw <- cbind(kaggle_matrix_raw, missing_mat)
}

kaggle_matrix_final <- kaggle_matrix_raw[, model_cols]

# predict
kaggle_probs <- predict(xgb_model, kaggle_matrix_final)

# generate submission file
submission <- data.frame(
  SK_ID_CURR = kaggle_test$SK_ID_CURR,
  TARGET = kaggle_probs
)

write.csv(submission, "kaggle_xgb_submission_final.csv", row.names = FALSE)

print_kbl(submission, caption = "Final Submission Preview")
```

The submission achieved a Private score of 0.70126 and a Public score of 0.70297 on Kaggle, indicating strong predictive performance.

# Conclusion

The analysis identified external credit scores as the most robust predictors of default risk. Higher scores strongly correlate with lower risk. Age and employment history provide secondary predictive value, while demographic factors like gender and income show negligible utility in isolation.

**Final Model Rankings:**
1.  **Gradient Boosting (XGBoost):** 0.742 AUC (Champion)
2.  **Logistic Regression:** 0.730 AUC
3.  **Decision Tree (Weighted):** 0.685 AUC
4.  **Random Forest:** 0.651 AUC

It is recommended that Home Credit utilize the XGBoost model to prioritize the riskiest applicants (e.g., the top 10%) for manual review, thereby optimizing resource allocation and minimizing potential defaults.